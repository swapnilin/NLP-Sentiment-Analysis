{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea7d857",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb4748",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861e4f2",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "\n",
    "### 1. Data Acquisition\n",
    "### 2. Data pre-processing and data cleaning\n",
    "### 3. Sentiment Analysis\n",
    "### 4. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719b494",
   "metadata": {},
   "source": [
    "In this notebook I am demonstrating how to pre-process and clean text data for Natural Lanuage Processing applications and how to perform Sentiment Analysis and Text Classification on the data.\n",
    "\n",
    "Pre-processing and data cleaning are crucial steps in any NLP (Natural Language Processing) project. These steps help to ensure that the data used in the project is of high quality, and ready to be used for further analysis and modeling. The following are the common pre-processing and data cleaning steps in an NLP project:\n",
    "\n",
    "1. **Data collection:** The first step is to collect the data that will be used in the project. This can be done from various sources such as text files, web pages, databases, etc.<br><br>\n",
    "\n",
    "2. **Data cleaning:** Remove any irrelevant or redundant information from the data such as special characters, punctuation marks, numbers, etc. This step also involves correcting any spelling mistakes or typos in the data ('recieve' > 'receive', 'brocoli' > 'broccoli').<br><br>\n",
    "\n",
    "3. **Text normalization:** Convert all the text data into a uniform format. This involves converting all the text to lowercase or uppercase, converting slang words or ackronyms ( e.g. lol, gn), expanding contractions (can't to can not), removing stop words (a, the, and, but), stemming or lemmatizing the words to reduce words to their base or root form.<br><br>\n",
    "\n",
    "  - **Stemming:** Stemming is the process of reducing words to their base form by removing suffixes. e.g. the root word in 'writing' and 'written' is 'write'. You get rid of 'ing', 'ed' ,'en'. Stemming algorithms are fast and efficient, but they can sometimes produce non-words or words with a different meaning than the original word.<br><br>\n",
    "\n",
    "  - **Lemmatization:** Lemmatization, on the other hand, is the process of reducing words to their base form using a morphological analysis of words. For e.g. the word 'better'. If we use a stemming algorithm to reduce this word to its base form, it will likely produce 'bett' as the stem. However, this stem is not a meaningful word and does not accurately represent the original word. Lemmatization on the other hand, will reduce the word to its base form or lemma of the word \"better\" would be \"good\".\n",
    "\n",
    "    The choice between stemming and lemmatization will depend on the specific requirements of   the NLP project and the trade-off between speed and accuracy.<br><br>\n",
    "\n",
    "4. **Tokenization:** Tokenization is the process of splitting the text data into smaller chunks or tokens, such as words, phrases, or sentences. This step helps in preparing the text data for further analysis and modeling.<br><br>\n",
    "\n",
    "5. **Text vectorization:** Text vectorization is the process of transforming data into a numerical format that can be used for analysis and modeling. This involves converting the text data into numerical vectors using techniques such as bag of words, TF-IDF, or word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93aa6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0645f5",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5416dd",
   "metadata": {},
   "source": [
    "## Different ways of importing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892193cc",
   "metadata": {},
   "source": [
    "1. Web scrapping\n",
    "2. Pdf/Word Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ea057",
   "metadata": {},
   "source": [
    "The `requests.get(url)` function is used to fetch the HTML content of the **url** specified. The HTML content is then passed to the **BeautifulSoup constructor**, which returns a **BeautifulSoup object** that can be used to parse the HTML.\n",
    "\n",
    "`soup.find_all(\"p\")[:3]` expression is used to find all the **<p>** elements in the HTML and select only the first three. The resulting list of elements is stored in the **paragraphs** variable which is a list of the 3 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb4788f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can. NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models. Together, these technologies enable computers to process human language in the form of text or voice data and to ‘understand’ its full meaning, complete with the speaker or writer’s intent and sentiment. NLP drives computer programs that translate text from one language to another, respond to spoken commands, and summarize large volumes of text rapidly—even in real time. There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems, digital assistants, speech-to-text dictation software, customer service chatbots, and other consumer conveniences. But NLP also plays a growing role in enterprise solutions that help streamline business operations, increase employee productivity, and simplify mission-critical business processes.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.ibm.com/topics/natural-language-processing#:~:text=the%20next%20step-,What%20is%20natural%20language%20processing%3F,same%20way%20human%20beings%20can.\"\n",
    "\n",
    "# this helps you to go to the website to fetch the content\n",
    "response = requests.get(url)\n",
    "\n",
    "# BeautifulSoup constructor takes the text as input and returns a BeautifulSoup object.\n",
    "# BeautifulSoup object makes it easier to parse and extract information from the HTML content.\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the first paragraph element in the HTML\n",
    "paragraphs = soup.find_all(\"p\")[:3]\n",
    "\n",
    "# Extract the text from the selected paragraphs and concatenate them\n",
    "combined_paragraph = ' '.join([p.text for p in paragraphs])\n",
    "\n",
    "print(combined_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the library\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfed4c3",
   "metadata": {},
   "source": [
    "Here I am using a sample pdf file that has a small explanation about NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce2e791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a field of study focused on making it # possible for \n",
      "computers to read, understand, and generate human language. NLP is an interdisciplinary \n",
      "field that combines computer science, AI, and linguistics.  \n",
      " \n",
      "The process of NLP includes several steps, such as tokenization, stop word removal, \n",
      "stemming and lemmatization, and more.  \n",
      " \n",
      "In tokenization, we break down the text into individual words, phrases, symbols, or other \n",
      "elements.  \n",
      " \n",
      "Stop word removal involves removing commonly used words such as and, the, a, etc. that \n",
      "do not contri bute much to the meaning of the text.  \n",
      " \n",
      "Stemming and lemmatization, on the other hand, are techniques to reduce words to their \n",
      "root form.  \n",
      " \n",
      "After the text has been cleaned and pre -processed, it can be used for various NLP tasks such \n",
      "as sentiment analysis, text classification, language translation, and more ! \n",
      " \n",
      "The effectiveness of these tasks greatly depends on the quality of the pre -processing step, \n",
      "making it a crucial step in NLP.  \n"
     ]
    }
   ],
   "source": [
    "# Open the PDF file\n",
    "with open(\"NLP.pdf\", \"rb\") as file:\n",
    "    # Create a PDF object\n",
    "    pdf = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Initialize a variable to store the extracted text\n",
    "    corpus = \"\"\n",
    "    \n",
    "    # Extract the text from each page of the PDF. We have only one page.\n",
    "    for page in pdf.pages:\n",
    "        corpus += page.extract_text()\n",
    "        \n",
    "    # Print the extracted text\n",
    "    print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece151bb",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb0152",
   "metadata": {},
   "source": [
    "### Convert all text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e4103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing (nlp) is a field of study focused on making it possible for \n",
      "computers to read, understand, and generate human language. nlp is an interdisciplinary \n",
      "field that combines computer science, artificial intelligence, and linguistics.  \n",
      " \n",
      "the process of nlp includes several steps, such as tokenization, stop word removal, \n",
      "stemming and lemmatization, and more.  \n",
      "in tokenization, we break down the text into individual words, phrases, symbols, or other \n",
      "elements.  \n",
      " \n",
      "stop word removal involves re moving commonly used words such as and, the, a, etc. that \n",
      "do not contribute much to the meaning of the text.  \n",
      "stemming and lemmatization, on the other hand, are techniques to reduce words to their \n",
      "root form.  \n",
      " \n",
      "after the text has been cleaned and pre -proces sed, it can be used for various nlp tasks such \n",
      "as sentiment analysis, text classification, language translation, and more.  \n",
      "the effectiveness of these tasks greatly depends on the quality of the pre -processing step, \n",
      "making it a crucial step in nlp.  \n"
     ]
    }
   ],
   "source": [
    "# Convert the text to lowercase\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42587bd1",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86c5f2",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "To tokenize the text, you can use the `word_tokenize` function from the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0838eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swapnil/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10d15ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'study', 'focused', 'on', 'making', 'it', 'possible', 'for', 'computers', 'to', 'read', ',', 'understand', ',', 'and', 'generate', 'human', 'language', '.', 'nlp', 'is', 'an', 'interdisciplinary', 'field', 'that', 'combines', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'linguistics', '.', 'the', 'process', 'of', 'nlp', 'includes', 'several', 'steps', ',', 'such', 'as', 'tokenization', ',', 'stop', 'word', 'removal', ',', 'stemming', 'and', 'lemmatization', ',', 'and', 'more', '.', 'in', 'tokenization', ',', 'we', 'break', 'down', 'the', 'text', 'into', 'individual', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'elements', '.', 'stop', 'word', 'removal', 'involves', 're', 'moving', 'commonly', 'used', 'words', 'such', 'as', 'and', ',', 'the', ',', 'a', ',', 'etc', '.', 'that', 'do', 'not', 'contribute', 'much', 'to', 'the', 'meaning', 'of', 'the', 'text', '.', 'stemming', 'and', 'lemmatization', ',', 'on', 'the', 'other', 'hand', ',', 'are', 'techniques', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', '.', 'after', 'the', 'text', 'has', 'been', 'cleaned', 'and', 'pre', '-proces', 'sed', ',', 'it', 'can', 'be', 'used', 'for', 'various', 'nlp', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'language', 'translation', ',', 'and', 'more', '.', 'the', 'effectiveness', 'of', 'these', 'tasks', 'greatly', 'depends', 'on', 'the', 'quality', 'of', 'the', 'pre', '-processing', 'step', ',', 'making', 'it', 'a', 'crucial', 'step', 'in', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text by words\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "    \n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a81b66cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural language processing (nlp) is a field of study focused on making it possible for \\ncomputers to read, understand, and generate human language.', 'nlp is an interdisciplinary \\nfield that combines computer science, artificial intelligence, and linguistics.', 'the process of nlp includes several steps, such as tokenization, stop word removal, \\nstemming and lemmatization, and more.', 'in tokenization, we break down the text into individual words, phrases, symbols, or other \\nelements.', 'stop word removal involves re moving commonly used words such as and, the, a, etc.', 'that \\ndo not contribute much to the meaning of the text.', 'stemming and lemmatization, on the other hand, are techniques to reduce words to their \\nroot form.', 'after the text has been cleaned and pre -proces sed, it can be used for various nlp tasks such \\nas sentiment analysis, text classification, language translation, and more.', 'the effectiveness of these tasks greatly depends on the quality of the pre -processing step, \\nmaking it a crucial step in nlp.']\n"
     ]
    }
   ],
   "source": [
    "#you can also do sentence tokenization\n",
    "stokens = nltk.sent_tokenize(corpus)\n",
    "print(stokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d73a63",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1234c6",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "To remove punctuations from the tokenized corpus, you can use the string module in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827b03d",
   "metadata": {},
   "source": [
    "The code below uses a list comprehension to iterate over the tokens and check if each token is not in the `string.punctuation` list. If a token is not in the string.punctuation list, it is added to a new list called tokens_without_punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d966e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'study', 'focused', 'on', 'making', 'it', 'possible', 'for', 'computers', 'to', 'read', 'understand', 'and', 'generate', 'human', 'language', 'nlp', 'is', 'an', 'interdisciplinary', 'field', 'that', 'combines', 'computer', 'science', 'artificial', 'intelligence', 'and', 'linguistics', 'the', 'process', 'of', 'nlp', 'includes', 'several', 'steps', 'such', 'as', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'and', 'lemmatization', 'and', 'more', 'in', 'tokenization', 'we', 'break', 'down', 'the', 'text', 'into', 'individual', 'words', 'phrases', 'symbols', 'or', 'other', 'elements', 'stop', 'word', 'removal', 'involves', 're', 'moving', 'commonly', 'used', 'words', 'such', 'as', 'and', 'the', 'a', 'etc', 'that', 'do', 'not', 'contribute', 'much', 'to', 'the', 'meaning', 'of', 'the', 'text', 'stemming', 'and', 'lemmatization', 'on', 'the', 'other', 'hand', 'are', 'techniques', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', 'after', 'the', 'text', 'has', 'been', 'cleaned', 'and', 'pre', '-proces', 'sed', 'it', 'can', 'be', 'used', 'for', 'various', 'nlp', 'tasks', 'such', 'as', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation', 'and', 'more', 'the', 'effectiveness', 'of', 'these', 'tasks', 'greatly', 'depends', 'on', 'the', 'quality', 'of', 'the', 'pre', '-processing', 'step', 'making', 'it', 'a', 'crucial', 'step', 'in', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuations from the tokens\n",
    "tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Print the tokens without punctuation\n",
    "print(tokens_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d90a18",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e52f1",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "To remove stop words from the tokenized corpus, you can use the `stopwords` corpus from the `nltk` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c176b0",
   "metadata": {},
   "source": [
    "The code below uses the `stopwords.words` function to get a list of stop words in English and stores the list in a set called stop_words. Then it uses a list comprehension to iterate over the tokens without punctuation and check if each token is not in the stop_words set. If a token is not in the stop_words set, it is added to a new list called tokens_without_stop_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b967b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'field', 'study', 'focused', 'making', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'nlp', 'interdisciplinary', 'field', 'combines', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'process', 'nlp', 'includes', 'several', 'steps', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'tokenization', 'break', 'text', 'individual', 'words', 'phrases', 'symbols', 'elements', 'stop', 'word', 'removal', 'involves', 'moving', 'commonly', 'used', 'words', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'techniques', 'reduce', 'words', 'root', 'form', 'text', 'cleaned', 'pre', '-proces', 'sed', 'used', 'various', 'nlp', 'tasks', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation', 'effectiveness', 'tasks', 'greatly', 'depends', 'quality', 'pre', '-processing', 'step', 'making', 'crucial', 'step', 'nlp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/swapnil/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get a list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokens\n",
    "tokens_without_stop_words = [token for token in tokens_without_punctuation if token not in stop_words]\n",
    "\n",
    "# Print the tokens without stop words\n",
    "print(tokens_without_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4096f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before removing stop words - 161\n",
      "Total words left after stop words removal - 91\n"
     ]
    }
   ],
   "source": [
    "print('Total words before removing stop words -',len(tokens_without_punctuation))\n",
    "print('Total words left after stop words removal -',len(tokens_without_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d4ad6",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e2c8c",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a44f268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'field', 'studi', 'focus', 'make', 'possibl', 'comput', 'read', 'understand', 'gener', 'human', 'languag', 'nlp', 'interdisciplinari', 'field', 'combin', 'comput', 'scienc', 'artifici', 'intellig', 'linguist', 'process', 'nlp', 'includ', 'sever', 'step', 'token', 'stop', 'word', 'remov', 'stem', 'lemmat', 'token', 'break', 'text', 'individu', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'remov', 'involv', 'move', 'commonli', 'use', 'word', 'etc', 'contribut', 'much', 'mean', 'text', 'stem', 'lemmat', 'hand', 'techniqu', 'reduc', 'word', 'root', 'form', 'text', 'clean', 'pre', '-proce', 'sed', 'use', 'variou', 'nlp', 'task', 'sentiment', 'analysi', 'text', 'classif', 'languag', 'translat', 'effect', 'task', 'greatli', 'depend', 'qualiti', 'pre', '-process', 'step', 'make', 'crucial', 'step', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens_without_stop_words]\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ace9e5",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31609e48",
   "metadata": {},
   "source": [
    "Observe how stemming creates non english words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f40322",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed999b",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The WordNet corpus is a lexical database of English words, developed by Princeton University. It groups words into sets of synonyms and provides short definitions and example sentences for each word. \n",
    "\n",
    "In lemmatization, the wordnet corpus is used to determine the base form of a word. For example, the lemma of \"better\" is \"good\". The lemmatizer uses the context of a word to determine its correct lemma based on its definition in the wordnet corpus. This is more sophisticated than stemming, which simply removes the suffixes from words without considering the meaning of the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00c1ae",
   "metadata": {},
   "source": [
    "In the code below, the `nltk.download('omw-1.4')` function downloads the Open Multilingual Wordnet (OMW) data package version 1.4 for the Natural Language Toolkit (NLTK) library in Python.\n",
    "\n",
    "The Open Multilingual Wordnet is a database of synonyms and related words in over 300 languages. It provides a hierarchical structure for words and concepts that allows for tasks such as word sense disambiguation and semantic similarity calculation.\n",
    "\n",
    "By downloading the OMW data package, you can use it within your NLTK-based project to perform various natural language processing tasks, such as lemmatization, word sense disambiguation, and word similarity computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3a0c46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'field', 'study', 'focused', 'making', 'possible', 'computer', 'read', 'understand', 'generate', 'human', 'language', 'nlp', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'process', 'nlp', 'includes', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'removal', 'involves', 'moving', 'commonly', 'used', 'word', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'technique', 'reduce', 'word', 'root', 'form', 'text', 'cleaned', 'pre', '-proces', 'sed', 'used', 'various', 'nlp', 'task', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation', 'effectiveness', 'task', 'greatly', 'depends', 'quality', 'pre', '-processing', 'step', 'making', 'crucial', 'step', 'nlp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/swapnil/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/swapnil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_without_stop_words]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3e00b",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01b3a5",
   "metadata": {},
   "source": [
    "### Handling acronyms and slang words\n",
    "\n",
    "create a dictionary that maps acronyms or slang words to their expanded form and then use this dictionary to replace the words in your tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55254fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'natural language processing', 'field', 'study', 'focused', 'making', 'possible', 'computer', 'read', 'understand', 'generate', 'human', 'language', 'natural language processing', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'process', 'natural language processing', 'includes', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'removal', 'involves', 'moving', 'commonly', 'used', 'word', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'technique', 'reduce', 'word', 'root', 'form', 'text', 'cleaned', 'pre', '-proces', 'sed', 'used', 'various', 'natural language processing', 'task', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation', 'effectiveness', 'task', 'greatly', 'depends', 'quality', 'pre', '-processing', 'step', 'making', 'crucial', 'step', 'natural language processing']\n"
     ]
    }
   ],
   "source": [
    "expanded_terms = {\n",
    "    'nlp': 'natural language processing'\n",
    "}\n",
    "\n",
    "expanded_tokens = [expanded_terms.get(token, token) for token in lemmatized_tokens]\n",
    "\n",
    "print(expanded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c027a",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8070e",
   "metadata": {},
   "source": [
    "The expanded_tokens list is created by iterating through lemmatized_tokens and using the `.get()` method to retrieve the expanded form of each token from the expanded_terms dictionary. If the expanded form of a token is not found in the dictionary, the original token is used.\n",
    "\n",
    "1. The expression before the **for** statement is executed for each item in the iterable specified after the **for** statement.\n",
    "2. In this case, the iterable is **lemmatized_tokens**, which is a list of words.\n",
    "3. `.get()` is a method that works on dictionaries in Python. It allows you to retrieve the value associated with a specified key in a dictionary. If the key you are searching for does not exist in the dictionary, the `.get()` method will return a default value that you can specify as a second argument to the method. If you don't specify a default value, the `.get()` method will return None by default. This can be useful when working with dictionaries as it allows you to access values in a safe and predictable manner, without the risk of encountering a KeyError if the key is not present in the dictionary.\n",
    "5. The result of each evaluation is added to a new list, which is assigned to the **expanded_acronyms** variable.\n",
    "6. The end result is a list of words where the acronyms have been expanded to their full form.\n",
    "\n",
    "\n",
    "In other words, the list comprehension is essentially going through each word in **lemmatized_tokens**, checking if it is an acronym, and if so, replacing it with its full form from **acronym_dict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a58cb",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46232979",
   "metadata": {},
   "source": [
    "### Fixing Typos\n",
    "To fix typos in our list of tokens, we can use spelling correction tools like the `Spellchecker` module in the `nltk` library or the autocorrect library. These tools work by comparing the words in your list of tokens to a dictionary of correctly spelled words, and making suggestions for corrected spellings based on the closest match.\n",
    "\n",
    "Use both one after the other for better results. The spelling correction is not always 100% accurate, and we may need to manually review the suggestions made by the tool to ensure that they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c0373c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f3e7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'study', 'focused', 'on', 'making', 'it', 'possible', 'for', 'computers', 'to', 'read', ',', 'understand', ',', 'and', 'generate', 'human', 'language', '.', 'nlp', 'is', 'an', 'interdisciplinary', 'field', 'that', 'combines', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'linguistics', '.', 'the', 'process', 'of', 'nlp', 'includes', 'several', 'steps', ',', 'such', 'as', 'tokenization', ',', 'stop', 'word', 'removal', ',', 'stemming', 'and', 'lemmatization', ',', 'and', 'more', '.', 'in', 'tokenization', ',', 'we', 'break', 'down', 'the', 'text', 'into', 'individual', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'elements', '.', 'stop', 'word', 'removal', 'involves', 're', 'moving', 'commonly', 'used', 'words', 'such', 'as', 'and', ',', 'the', ',', 'a', ',', 'etc', '.', 'that', 'do', 'not', 'contribute', 'much', 'to', 'the', 'meaning', 'of', 'the', 'text', '.', 'stemming', 'and', 'lemmatization', ',', 'on', 'the', 'other', 'hand', ',', 'are', 'techniques', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', '.', 'after', 'the', 'text', 'has', 'been', 'cleaned', 'and', 'pre', '-proces', 'sed', ',', 'it', 'can', 'be', 'used', 'for', 'various', 'nlp', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'language', 'translation', ',', 'and', 'more', '.', 'the', 'effectiveness', 'of', 'these', 'tasks', 'greatly', 'depends', 'on', 'the', 'quality', 'of', 'the', 'pre', '-processing', 'step', ',', 'making', 'it', 'a', 'crucial', 'step', 'in', 'nlp', '.']\n",
      "Corrected tokens: ['natural', 'language', 'processing', 'natural language processing', 'field', 'study', 'focused', 'making', 'possible', 'computer', 'read', 'understand', 'generate', 'human', 'language', 'natural language processing', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'process', 'natural language processing', 'includes', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'removal', 'involves', 'moving', 'commonly', 'used', 'word', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'technique', 'reduce', 'word', 'root', 'form', 'text', 'cleaned', 'pre', 'process', 'sed', 'used', 'various', 'natural language processing', 'task', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation', 'effectiveness', 'task', 'greatly', 'depends', 'quality', 'pre', 'processing', 'step', 'making', 'crucial', 'step', 'natural language processing']\n"
     ]
    }
   ],
   "source": [
    "import autocorrect\n",
    "\n",
    "spell = autocorrect.Speller(lang='en')\n",
    "\n",
    "#tokens = ['This', 'is', 'speling', 'misstake']\n",
    "corrected_tokens = [spell.autocorrect_word(token) for token in expanded_tokens]\n",
    "print('Original tokens:', tokens)\n",
    "print('Corrected tokens:', corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b5b35",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7592c0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc6305",
   "metadata": {},
   "source": [
    "To perform sentiment analysis on the list of corrected tokens, we can use various libraries in Python such as:\n",
    "\n",
    "1. **nltk:** The Natural Language Toolkit (nltk) provides a SentimentIntensityAnalyzer class that can be used to calculate the sentiment of a piece of text. We can use the `polarity_scores()` method to obtain the sentiment scores.<br><br>\n",
    "\n",
    "2. **TextBlob:** TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdf84331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db96d3",
   "metadata": {},
   "source": [
    "This would give you a sentiment polarity score, where a score closer to 1 indicates a positive sentiment, a score closer to -1 indicates a negative sentiment, and a score closer to 0 indicates a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a94a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.04375000000000001, subjectivity=0.4906250000000001)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \" \".join(corrected_tokens)\n",
    "\n",
    "analysis = TextBlob(text)\n",
    "\n",
    "print(analysis.sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcce773",
   "metadata": {},
   "source": [
    "The subjectivity score in sentiment analysis refers to the subjectivity of the text, where 0 is considered to be objective and 1 is considered to be subjective. The subjectivity score of 0.4906250000000001 means that the text has a somewhat subjective tone, with a leaning towards being objective.\n",
    "\n",
    "In other words, the text is not purely subjective and doesn't express personal opinions, feelings, emotions or subjective interpretations, but it has some subjectivity and is not entirely neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be224d",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaedfa8",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652416d",
   "metadata": {},
   "source": [
    "The \"VADER (Valence Aware Dictionary and sEntiment Reasoner)\" lexicon, is a pre-trained lexicon and rule-based sentiment analysis tool. It is part of the Natural Language Toolkit (nltk) library in Python, and is used to determine the sentiment of text data by analyzing the words used in the text and the context in which they are used.\n",
    "\n",
    "The VADER lexicon contains a list of words and their associated sentiment scores, which are based on the words' connotations, intensities, and tendencies to appear in positive, neutral, or negative contexts. When analyzing text, the sentiment analysis tool looks at each word in the text and uses the sentiment scores in the VADER lexicon to determine the overall sentiment of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a750d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/swapnil/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate the sentiment score for each token\n",
    "scores = [sia.polarity_scores(token) for token in corrected_tokens]\n",
    "\n",
    "# Calculate the average sentiment score for the text\n",
    "sentiment_score = sum(score['compound'] for score in scores) / len(scores)\n",
    "\n",
    "# Check the sentiment of the text\n",
    "if sentiment_score >= 0.05:\n",
    "    sentiment = \"positive\"\n",
    "elif sentiment_score <= -0.05:\n",
    "    sentiment = \"negative\"\n",
    "else:\n",
    "    sentiment = \"neutral\"\n",
    "\n",
    "# Print the sentiment of the text\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260c813",
   "metadata": {},
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd570e",
   "metadata": {},
   "source": [
    "#### Both the libraries are generating a neutral sentiment output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b39b55",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47cd89",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "Text classification is the process of assigning predefined categories or labels to a document or a piece of text. This is a fundamental problem in Natural Language Processing (NLP) and it is often used in sentiment analysis, spam filtering, and topic classification, among other applications.\n",
    "\n",
    "In order to perform text classification, we need a labeled dataset. The labeled dataset is then used to train a machine learning model to predict the category of new, unseen text data. The labels in the training dataset are used to train the model to associate certain words, phrases, or patterns of words and phrases with specific categories. Once the model is trained, it can then be used to make predictions on new, unseen text data based on the patterns it learned during training.\n",
    "\n",
    "When we don't have a labeled dataset, we can still perform text classification using unsupervised learning techniques. One such technique is clustering, where the algorithm tries to group similar text documents together. Another approach is topic modeling, where the algorithm identifies the topics present in a text corpus and categorizes each document into one or more of those topics. However, these techniques are less accurate compared to supervised learning techniques when used for text classification, as they don't have the benefit of training on a labeled dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
